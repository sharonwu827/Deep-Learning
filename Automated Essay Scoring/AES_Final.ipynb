{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AES - Final.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sharonwu827/Deep-Learning/blob/master/AES_Final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T-9jqlYZCY0g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc0e6d7c-630d-4c88-b29c-6a117f0dfb8d"
      },
      "source": [
        "#load dataset\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KIZyNyhtCdKu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "85a12afa-ef14-4b6a-c079-e79d3397761a"
      },
      "source": [
        "# import package\n",
        "!pip install transformers\n",
        "!pip install bert-tensorflow \n",
        "!pip install pyenchant\n",
        "!pip install -U pip setuptools wheel\n",
        "!pip install -U spacy\n",
        "!sudo apt-get install libenchant1c2a\n",
        "!python -m spacy download en_core_web_sm\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import numpy as np \n",
        "from numpy import asarray\n",
        "import pandas as pd \n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.model_selection import train_test_split, KFold\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk import word_tokenize\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "\n",
        "import re\n",
        "import time\n",
        "import torch\n",
        "import transformers as ppb\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import transformers\n",
        "from transformers import BertTokenizer, BertModel, AutoModel, BertTokenizerFast,BertForSequenceClassification\n",
        "\n",
        "# Keras functional API\n",
        "from keras.models import Sequential, Model, load_model, model_from_config\n",
        "from keras.layers import Dense, Dropout, Embedding, LSTM, Input, Flatten, Lambda\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "import keras.backend as K\n",
        "\n",
        "import os\n",
        "import csv\n",
        "import copy\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "\n",
        "import enchant #  Enchant spellchecking library\n",
        "import spacy\n",
        "import re\n",
        "\n",
        "import urllib.request\n",
        "from bs4 import BeautifulSoup\n",
        "#!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "#!unzip -q glove.6B.zip"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.8.2)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from transformers) (3.13)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (21.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: huggingface-hub==0.0.12 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.12)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.45)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.6.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub==0.0.12->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.5.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
            "Requirement already satisfied: bert-tensorflow in /usr/local/lib/python3.7/dist-packages (1.0.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from bert-tensorflow) (1.15.0)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
            "Requirement already satisfied: pyenchant in /usr/local/lib/python3.7/dist-packages (3.2.1)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
            "Requirement already satisfied: pip in /usr/local/lib/python3.7/dist-packages (21.1.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (57.2.0)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.7/dist-packages (0.36.2)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.7/dist-packages (3.1.0)\n",
            "Requirement already satisfied: thinc<8.1.0,>=8.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy) (8.0.7)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy) (57.2.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (21.0)\n",
            "Requirement already satisfied: typer<0.4.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.3.2)\n",
            "Requirement already satisfied: typing-extensions<4.0.0.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.7.4.3)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.8.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.11.3)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.4 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.4)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.8.2)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.6.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.19.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.23.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (4.41.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.8)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.4.1)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.5)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.4->spacy) (3.5.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy) (2.4.7)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy) (5.1.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2021.5.30)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
            "Requirement already satisfied: click<7.2.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.4.0,>=0.3.0->spacy) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy) (2.0.1)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "libenchant1c2a is already the newest version (1.6.0-11.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 39 not upgraded.\n",
            "2021-07-16 04:44:00.496293: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
            "Collecting en-core-web-sm==3.1.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.1.0/en_core_web_sm-3.1.0-py3-none-any.whl (13.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 13.6 MB 5.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy<3.2.0,>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from en-core-web-sm==3.1.0) (3.1.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (2.11.3)\n",
            "Requirement already satisfied: typer<0.4.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (0.3.2)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (3.0.8)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (0.6.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (57.2.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (0.8.2)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (1.19.5)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (0.4.1)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (1.8.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (2.4.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (21.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (2.0.5)\n",
            "Requirement already satisfied: thinc<8.1.0,>=8.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (8.0.7)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (2.23.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (1.0.5)\n",
            "Requirement already satisfied: typing-extensions<4.0.0.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (3.7.4.3)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (3.0.5)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (2.0.4)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (4.41.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.4->spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (3.5.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (2.4.7)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (5.1.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (2021.5.30)\n",
            "Requirement already satisfied: click<7.2.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.4.0,>=0.3.0->spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (2.0.1)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DxKHFvCoCjCH"
      },
      "source": [
        "train = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/asap-aes/training_set_rel3.tsv\",sep='\\t', encoding='ISO-8859-1')\n",
        "dev = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/asap-aes/valid_set.tsv\",sep='\\t', encoding='ISO-8859-1')\n",
        "test = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/asap-aes/test_set.tsv\",sep='\\t', encoding='ISO-8859-1')"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "teoEWhRFES5B"
      },
      "source": [
        "prompt = pd.DataFrame({\"essay_set\":[1,2,3,4,5,6,7,8],\n",
        "                       \"prompt\":[\"More and more people use computers, but not everyone agrees that this benefits society. Those who support advances in technology believe that computers have a positive effect on people. They teach hand-eye coordination, give people the ability to learn about faraway places and people, and even allow people to talk online with other people. Others have different ideas. Some experts are concerned that people are spending too much time on their computers and less time exercising, enjoying nature, and interacting with family and friends. Write a letter to your local newspaper in which you state your opinion on the effects computers have on people. Persuade the readers to agree with you.\",\n",
        "                                \"Write a persuasive essay to a newspaper reflecting your vies on censorship in libraries. Do you believe that certain materials, such as books, music, movies, magazines, etc., should be removed from the shelves if they are found offensive? Support your position with convincing arguments from your own experience, observations, and/or reading.\",\n",
        "                                \"Write a response that explains how the features of the setting affect the cyclist. In your response, include examples from the essay that support your conclusion.\",\n",
        "                                \"Write a response that explains why the author concludes the story with this paragraph. In your response, include details and examples from the story that support your ideas.\",\n",
        "                                \"Describe the mood created by the author in the memoir. Support your answer with relevant and specific information from the memoir.\",\n",
        "                                \"Based on the excerpt, describe the obstacles the builders of the Empire State Building faced in attempting to allow dirigibles to dock there. Support your answer with relevant and specific information from the excerpt.\",\n",
        "                                \"Write about patience. Being patient means that you are understanding and tolerant. A patient person experience difficulties without complaining.Do only one of the following: write a story about a time when you were patient OR write a story about a time when someone you know was patient OR write a story in your own way about patience.\",\n",
        "                                \"We all understand the benefits of laughter. For example, someone once said, “Laughter is the shortest distance between two people.” Many other people believe that laughter is an important part of any relationship. Tell a true story in which laughter was one element or part.\"]})"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GI8lLiFQCnN3"
      },
      "source": [
        "## Data processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PjaoZsfOCkY6"
      },
      "source": [
        "def preprocess(df):\n",
        "  df['normalized_score'] = df['domain1_score'] / df.groupby('essay_set')['domain1_score'].transform('max')\n",
        "\n",
        "preprocess(train)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uYA_qKTmsG8N"
      },
      "source": [
        "def clean_anonymization(essay):\n",
        "  '''\n",
        "  function to remove the anoymaization\n",
        "  '''\n",
        "  res=[]\n",
        "  # Lowercase\n",
        "  essay = essay.lower()\n",
        "  # Replace everything not a letter or apostrophe with a space\n",
        "  essay = re.sub('[^a-zA-Z\\']', ' ', essay)\n",
        "  # Remove single letter words\n",
        "  essay = ' '.join( [w for w in essay.split() if len(w)>1] )\n",
        "  for i in essay.split():\n",
        "    if i.startswith(\"@\"):\n",
        "      continue\n",
        "    else:\n",
        "      res.append(i)\n",
        "  return ' '.join(res)\n",
        "\n",
        "train['essay']=train['essay'].apply(lambda x:clean_anonymization(x))\n",
        "# also remove from dev and test\n",
        "dev['essay']=dev['essay'].apply(lambda x:clean_anonymization(x))\n",
        "test['essay']=test['essay'].apply(lambda x:clean_anonymization(x))"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ic3FQ2qpUJNv"
      },
      "source": [
        "#-------------- change here------------------#\n",
        "X = train[:1000][['essay']]\n",
        "y = train[:1000]['normalized_score']"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uiQ6QTXaDv26"
      },
      "source": [
        "## Load Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ai2KyNAAPJoY",
        "outputId": "5e387f66-ad82-4f85-a9e0-d4d5549c2894"
      },
      "source": [
        "# Load pre-trained model (weights)\n",
        "model_class, tokenizer_class, pretrained_weights = (ppb.BertModel, ppb.BertTokenizer, 'bert-base-uncased')\n",
        "tokenizer = tokenizer_class.from_pretrained(pretrained_weights)\n",
        "model = model_class.from_pretrained(pretrained_weights)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1EZe1CWxRKdA"
      },
      "source": [
        "def lstm_model():\n",
        "    \"\"\"\n",
        "    Define the model\n",
        "    \"\"\"\n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(LSTM(300, dropout=0.4, recurrent_dropout=0.4, input_shape=[1,768], return_sequences=True))\n",
        "    model.add(LSTM(64, recurrent_dropout=0.4))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(1, activation='relu'))\n",
        "\n",
        "    model.compile(loss='mean_squared_error', optimizer='adam', metrics=['mae'])\n",
        "    model.summary()\n",
        "\n",
        "    return model"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LYn0N0n4T47c"
      },
      "source": [
        "## Semantic Score"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "y51scnYKN0Z_",
        "outputId": "1a09d046-43ee-4e61-d76c-7a3666ccb2bf"
      },
      "source": [
        "total_loss = []\n",
        "results = []\n",
        "prediction_list = []\n",
        "\n",
        "cv = KFold(n_splits=5, shuffle=True)\n",
        "cv_data = cv.split(X)\n",
        "\n",
        "fold_cnt =1\n",
        "cuda = torch.device('cuda')\n",
        "\n",
        "with torch.cuda.device(cuda):\n",
        "  for traincv, testcv in cv_data:\n",
        "    torch.cuda.empty_cache()\n",
        "    print(\"Fold {}\".format(fold_cnt))\n",
        "    \n",
        "    # get the train and test from the dataset.\n",
        "    X_train, X_test, y_train, y_test = X.iloc[traincv], X.iloc[testcv], y.iloc[traincv], y.iloc[testcv]\n",
        "    train_essays = X_train['essay']\n",
        "    test_essays = X_test['essay']\n",
        "    \n",
        "    tokenized_train = train_essays.apply((lambda x: tokenizer.encode(x, add_special_tokens=True,max_length=200)))\n",
        "    tokenized_test = test_essays.apply((lambda x: tokenizer.encode(x, add_special_tokens=True ,max_length=200)))\n",
        "    \n",
        "    # train dataset\n",
        "    max_len = 0\n",
        "    for i in tokenized_train.values:\n",
        "      if len(i) > max_len:\n",
        "        max_len = len(i)\n",
        "    padded_train = np.array([i + [0]*(max_len-len(i)) for i in tokenized_train.values])\n",
        "    \n",
        "    attention_mask_train = np.where(padded_train != 0, 1, 0)\n",
        "    train_input_ids = torch.tensor(padded_train)\n",
        "    train_attention_mask = torch.tensor(attention_mask_train)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "      outputs_train  = model(train_input_ids, attention_mask=train_attention_mask)\n",
        "      last_hidden_states_train = outputs_train[0]  # get the last hidden state\n",
        "    train_features = last_hidden_states_train[:,0,:].numpy()\n",
        "    \n",
        "    ## test dataset\n",
        "    max_len = 0\n",
        "    for i in tokenized_test.values:\n",
        "      if len(i) > max_len:\n",
        "        max_len = len(i)\n",
        "    padded_test = np.array([i + [0]*(max_len-len(i)) for i in tokenized_test.values])\n",
        "    \n",
        "    attention_mask_test = np.where(padded_test != 0, 1, 0)\n",
        "    test_input_ids = torch.tensor(padded_test)  \n",
        "    test_attention_mask = torch.tensor(attention_mask_test)\n",
        "\n",
        "    with torch.no_grad():\n",
        "      outputs_test  = model(test_input_ids, attention_mask=test_attention_mask)\n",
        "      last_hidden_states_test = outputs_test[0]  # get the last hidden state\n",
        "    test_features = last_hidden_states_test[:,0,:].numpy()\n",
        "    train_x,train_y = train_features.shape\n",
        "    test_x,test_y = test_features.shape\n",
        "\n",
        "    x_train_reshaped = np.reshape(train_features,(train_x,1,train_y))\n",
        "    x_test_reshaped = np.reshape(test_features,(test_x,1,test_y))\n",
        "\n",
        "    lstm = lstm_model()\n",
        "    lstm.fit(x_train_reshaped, y_train, batch_size=128, epochs=70)\n",
        "    \n",
        "    y_pred = lstm.predict(x_test_reshaped)\n",
        "    \n",
        "    # evaluate the model\n",
        "    result = mean_squared_error(y_test.values,y_pred)\n",
        "    print(\"MSE: {}\".format(result))\n",
        "    results.append(result)\n",
        "    fold_cnt +=1\n",
        "\n",
        "    tf.keras.backend.clear_session()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Fold 1\n",
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm (LSTM)                  (None, 1, 300)            1282800   \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 64)                93440     \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 1)                 65        \n",
            "=================================================================\n",
            "Total params: 1,376,305\n",
            "Trainable params: 1,376,305\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/70\n",
            "7/7 [==============================] - 22s 10ms/step - loss: 0.2258 - mae: 0.4036\n",
            "Epoch 2/70\n",
            "7/7 [==============================] - 0s 9ms/step - loss: 0.0506 - mae: 0.1797\n",
            "Epoch 3/70\n",
            "7/7 [==============================] - 0s 10ms/step - loss: 0.0289 - mae: 0.1353\n",
            "Epoch 4/70\n",
            "7/7 [==============================] - 0s 10ms/step - loss: 0.0284 - mae: 0.1343\n",
            "Epoch 5/70\n",
            "7/7 [==============================] - 0s 9ms/step - loss: 0.0264 - mae: 0.1294\n",
            "Epoch 6/70\n",
            "7/7 [==============================] - 0s 10ms/step - loss: 0.0252 - mae: 0.1238\n",
            "Epoch 7/70\n",
            "7/7 [==============================] - 0s 11ms/step - loss: 0.0217 - mae: 0.1151\n",
            "Epoch 8/70\n",
            "7/7 [==============================] - 0s 9ms/step - loss: 0.0220 - mae: 0.1160\n",
            "Epoch 9/70\n",
            "7/7 [==============================] - 0s 10ms/step - loss: 0.0200 - mae: 0.1123\n",
            "Epoch 10/70\n",
            "7/7 [==============================] - 0s 10ms/step - loss: 0.0206 - mae: 0.1118\n",
            "Epoch 11/70\n",
            "7/7 [==============================] - 0s 10ms/step - loss: 0.0202 - mae: 0.1117\n",
            "Epoch 12/70\n",
            "7/7 [==============================] - 0s 10ms/step - loss: 0.0179 - mae: 0.1028\n",
            "Epoch 13/70\n",
            "7/7 [==============================] - 0s 9ms/step - loss: 0.0174 - mae: 0.1043\n",
            "Epoch 14/70\n",
            "7/7 [==============================] - 0s 9ms/step - loss: 0.0179 - mae: 0.1049\n",
            "Epoch 15/70\n",
            "7/7 [==============================] - 0s 9ms/step - loss: 0.0193 - mae: 0.1123\n",
            "Epoch 16/70\n",
            "7/7 [==============================] - 0s 10ms/step - loss: 0.0180 - mae: 0.1058\n",
            "Epoch 17/70\n",
            "7/7 [==============================] - 0s 11ms/step - loss: 0.0189 - mae: 0.1114\n",
            "Epoch 18/70\n",
            "7/7 [==============================] - 0s 10ms/step - loss: 0.0177 - mae: 0.1056\n",
            "Epoch 19/70\n",
            "7/7 [==============================] - 0s 10ms/step - loss: 0.0190 - mae: 0.1105\n",
            "Epoch 20/70\n",
            "7/7 [==============================] - 0s 11ms/step - loss: 0.0165 - mae: 0.1018\n",
            "Epoch 21/70\n",
            "7/7 [==============================] - 0s 10ms/step - loss: 0.0169 - mae: 0.1037\n",
            "Epoch 22/70\n",
            "7/7 [==============================] - 0s 10ms/step - loss: 0.0174 - mae: 0.1064\n",
            "Epoch 23/70\n",
            "7/7 [==============================] - 0s 10ms/step - loss: 0.0177 - mae: 0.1044\n",
            "Epoch 24/70\n",
            "7/7 [==============================] - 0s 10ms/step - loss: 0.0158 - mae: 0.1003\n",
            "Epoch 25/70\n",
            "7/7 [==============================] - 0s 10ms/step - loss: 0.0169 - mae: 0.1038\n",
            "Epoch 26/70\n",
            "7/7 [==============================] - 0s 10ms/step - loss: 0.0173 - mae: 0.1042\n",
            "Epoch 27/70\n",
            "7/7 [==============================] - 0s 11ms/step - loss: 0.0165 - mae: 0.1040\n",
            "Epoch 28/70\n",
            "7/7 [==============================] - 0s 10ms/step - loss: 0.0166 - mae: 0.1014\n",
            "Epoch 29/70\n",
            "7/7 [==============================] - 0s 10ms/step - loss: 0.0163 - mae: 0.1022\n",
            "Epoch 30/70\n",
            "7/7 [==============================] - 0s 10ms/step - loss: 0.0151 - mae: 0.0979\n",
            "Epoch 31/70\n",
            "7/7 [==============================] - 0s 10ms/step - loss: 0.0171 - mae: 0.1016\n",
            "Epoch 32/70\n",
            "7/7 [==============================] - 0s 10ms/step - loss: 0.0154 - mae: 0.0993\n",
            "Epoch 33/70\n",
            "7/7 [==============================] - 0s 11ms/step - loss: 0.0165 - mae: 0.1012\n",
            "Epoch 34/70\n",
            "7/7 [==============================] - 0s 9ms/step - loss: 0.0160 - mae: 0.0998\n",
            "Epoch 35/70\n",
            "7/7 [==============================] - 0s 9ms/step - loss: 0.0154 - mae: 0.0987\n",
            "Epoch 36/70\n",
            "7/7 [==============================] - 0s 9ms/step - loss: 0.0151 - mae: 0.0972\n",
            "Epoch 37/70\n",
            "7/7 [==============================] - 0s 10ms/step - loss: 0.0142 - mae: 0.0958\n",
            "Epoch 38/70\n",
            "7/7 [==============================] - 0s 10ms/step - loss: 0.0143 - mae: 0.0956\n",
            "Epoch 39/70\n",
            "7/7 [==============================] - 0s 11ms/step - loss: 0.0167 - mae: 0.1019\n",
            "Epoch 40/70\n",
            "7/7 [==============================] - 0s 10ms/step - loss: 0.0152 - mae: 0.0987\n",
            "Epoch 41/70\n",
            "7/7 [==============================] - 0s 10ms/step - loss: 0.0156 - mae: 0.0994\n",
            "Epoch 42/70\n",
            "7/7 [==============================] - 0s 11ms/step - loss: 0.0141 - mae: 0.0930\n",
            "Epoch 43/70\n",
            "7/7 [==============================] - 0s 11ms/step - loss: 0.0153 - mae: 0.0976\n",
            "Epoch 44/70\n",
            "7/7 [==============================] - 0s 10ms/step - loss: 0.0145 - mae: 0.0937\n",
            "Epoch 45/70\n",
            "7/7 [==============================] - 0s 10ms/step - loss: 0.0156 - mae: 0.1002\n",
            "Epoch 46/70\n",
            "7/7 [==============================] - 0s 10ms/step - loss: 0.0153 - mae: 0.0980\n",
            "Epoch 47/70\n",
            "7/7 [==============================] - 0s 10ms/step - loss: 0.0141 - mae: 0.0932\n",
            "Epoch 48/70\n",
            "7/7 [==============================] - 0s 10ms/step - loss: 0.0137 - mae: 0.0917\n",
            "Epoch 49/70\n",
            "7/7 [==============================] - 0s 10ms/step - loss: 0.0149 - mae: 0.0948\n",
            "Epoch 50/70\n",
            "7/7 [==============================] - 0s 10ms/step - loss: 0.0156 - mae: 0.0989\n",
            "Epoch 51/70\n",
            "7/7 [==============================] - 0s 10ms/step - loss: 0.0150 - mae: 0.0970\n",
            "Epoch 52/70\n",
            "7/7 [==============================] - 0s 11ms/step - loss: 0.0163 - mae: 0.1019\n",
            "Epoch 53/70\n",
            "7/7 [==============================] - 0s 9ms/step - loss: 0.0140 - mae: 0.0953\n",
            "Epoch 54/70\n",
            "7/7 [==============================] - 0s 9ms/step - loss: 0.0123 - mae: 0.0879\n",
            "Epoch 55/70\n",
            "7/7 [==============================] - 0s 9ms/step - loss: 0.0133 - mae: 0.0905\n",
            "Epoch 56/70\n",
            "7/7 [==============================] - 0s 10ms/step - loss: 0.0133 - mae: 0.0919\n",
            "Epoch 57/70\n",
            "7/7 [==============================] - 0s 10ms/step - loss: 0.0139 - mae: 0.0926\n",
            "Epoch 58/70\n",
            "7/7 [==============================] - 0s 11ms/step - loss: 0.0132 - mae: 0.0919\n",
            "Epoch 59/70\n",
            "7/7 [==============================] - 0s 10ms/step - loss: 0.0138 - mae: 0.0954\n",
            "Epoch 60/70\n",
            "7/7 [==============================] - 0s 10ms/step - loss: 0.0138 - mae: 0.0950\n",
            "Epoch 61/70\n",
            "7/7 [==============================] - 0s 10ms/step - loss: 0.0123 - mae: 0.0880\n",
            "Epoch 62/70\n",
            "7/7 [==============================] - 0s 10ms/step - loss: 0.0132 - mae: 0.0898\n",
            "Epoch 63/70\n",
            "7/7 [==============================] - 0s 9ms/step - loss: 0.0127 - mae: 0.0888\n",
            "Epoch 64/70\n",
            "7/7 [==============================] - 0s 10ms/step - loss: 0.0124 - mae: 0.0895\n",
            "Epoch 65/70\n",
            "7/7 [==============================] - 0s 9ms/step - loss: 0.0128 - mae: 0.0872\n",
            "Epoch 66/70\n",
            "7/7 [==============================] - 0s 10ms/step - loss: 0.0134 - mae: 0.0921\n",
            "Epoch 67/70\n",
            "7/7 [==============================] - 0s 10ms/step - loss: 0.0123 - mae: 0.0890\n",
            "Epoch 68/70\n",
            "7/7 [==============================] - 0s 10ms/step - loss: 0.0141 - mae: 0.0952\n",
            "Epoch 69/70\n",
            "7/7 [==============================] - 0s 9ms/step - loss: 0.0133 - mae: 0.0929\n",
            "Epoch 70/70\n",
            "7/7 [==============================] - 0s 9ms/step - loss: 0.0130 - mae: 0.0910\n",
            "MSE: 0.011193441157628077\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-904bdda52754>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"MSE: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m     \u001b[0mfold_count\u001b[0m \u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclear_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'fold_count' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "02toUwNMW7L2"
      },
      "source": [
        "total_loss.append(results)\n",
        "print(\"mean squared error is : {}\".format(np.mean(np.asarray(results))))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "irNougvdN0ht"
      },
      "source": [
        "semantic_score = y_pred"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P61-vXV8YXJI"
      },
      "source": [
        "semantic_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gs3BxpdxUJ2T"
      },
      "source": [
        "## prompt relevant score"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ADHv-0u6nvDG"
      },
      "source": [
        "words = re.compile(r\"\\w+\",re.I)\n",
        "stopword = stopwords.words('english')\n",
        "from nltk.stem.porter import *\n",
        "stemmer = PorterStemmer()\n",
        "import gensim\n",
        "from gensim import corpora\n",
        "from sklearn.metrics.pairwise import cosine_similarity as cs\n",
        "from sklearn.metrics.pairwise import manhattan_distances as md\n",
        "from sklearn.metrics.pairwise import euclidean_distances as ed\n",
        "from sklearn.metrics import jaccard_similarity_score as jsc\n",
        "from sklearn.neighbors import DistanceMetric\n",
        "from sklearn.preprocessing import MinMaxScaler"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4UB4OQNkUal9"
      },
      "source": [
        "train_p = train.merge(prompt,on='essay_set',how='left')\n",
        "train_p = train_p[['essay','normalized_score','prompt']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8v8TJaHsUay9"
      },
      "source": [
        "def tokenize(df):\n",
        "    essay_tokenized = []\n",
        "    prompt_tokenized = []\n",
        "\n",
        "    for q in df.essay.tolist():\n",
        "        essay_tokenized.append([stemmer.stem(i.lower()) for i in words.findall(q) if i not in stopword])\n",
        "\n",
        "    for q in df.prompt.tolist():\n",
        "        prompt_tokenized.append([stemmer.stem(i.lower()) for i in words.findall(q) if i not in stopword])\n",
        "\n",
        "    df[\"essay_tokenized\"] = essay_tokenized\n",
        "    df[\"prompt_tokenized\"] = prompt_tokenized"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rqjz9t0Wnnq8"
      },
      "source": [
        "tokenize(train_p)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rw1C1zR1nxOi"
      },
      "source": [
        "minkowski_dis = DistanceMetric.get_metric('minkowski')\n",
        "mms_scale_man = MinMaxScaler()\n",
        "mms_scale_euc = MinMaxScaler()\n",
        "mms_scale_mink = MinMaxScaler()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZQoTLzRknxXb"
      },
      "source": [
        "def get_similarity_values(q1_csc, q2_csc):\n",
        "    cosine_sim = []\n",
        "    manhattan_dis = []\n",
        "    eucledian_dis = []\n",
        "    jaccard_dis = []\n",
        "    minkowsk_dis = []\n",
        "    \n",
        "    for i,j in zip(q1_csc, q2_csc):\n",
        "        sim = cs(i,j)\n",
        "        cosine_sim.append(sim[0][0])\n",
        "        sim = md(i,j)\n",
        "        manhattan_dis.append(sim[0][0])\n",
        "        sim = ed(i,j)\n",
        "        eucledian_dis.append(sim[0][0])\n",
        "        i_ = i.toarray()\n",
        "        j_ = j.toarray()\n",
        "        try:\n",
        "            sim = jsc(i_,j_)\n",
        "            jaccard_dis.append(sim)\n",
        "        except:\n",
        "            jaccard_dis.append(0)\n",
        "        sim = minkowski_dis.pairwise(i_,j_)\n",
        "        minkowsk_dis.append(sim[0][0])\n",
        "    return cosine_sim, manhattan_dis, eucledian_dis, jaccard_dis, minkowsk_dis    \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sz-9TfCeGOt0"
      },
      "source": [
        "## Handcrafted Features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LRRKa3ZJGUGl"
      },
      "source": [
        "def get_correct_and_incorrect_spelling(df):\n",
        "  \"\"\"\n",
        "  Function that measures lexical diversity which is the ratio of total words to unique words\n",
        "  \"\"\"\n",
        "  # Load spaCy model\n",
        "  nlp = spacy.load('en_core_web_sm')\n",
        "  # load the enchant dictionary\n",
        "  d = enchant.Dict(\"en_US\")\n",
        "  \n",
        "  incorrect_spelling = []\n",
        "  correct_spelling = []\n",
        "  \n",
        "  for essay in df['essay']:\n",
        "    individual = []\n",
        "    for word in essay.split():\n",
        "      individual.append(d.check(word))                                                                         \n",
        "    incorrect_spelling.append(individual.count(False))\n",
        "    correct_spelling.append(individual.count(True))\n",
        "  return incorrect_spelling, correct_spelling\n",
        "\n",
        "\n",
        "def get_lexical_diversity(df):\n",
        "    \"\"\"\n",
        "    Function that measures lexical diversity which is\n",
        "    The ratio of total words to unique words\n",
        "    \"\"\"\n",
        "    diversity = []\n",
        "    for essay in df['essay']:\n",
        "      diversity.append(round(len(tk.word_tokenize(essay)) / float(len(set(tk.word_tokenize(essay)))), 2))\n",
        "    return diversity\n",
        "\n",
        "\n",
        "def get_list_of_number_of_pos(df):\n",
        "    \"\"\"\n",
        "    Function that parses the essay for each words POS\n",
        "    Returns tuples containg for now, nouns, verbs, adverbs and adjectives\n",
        "    \"\"\"\n",
        "    noun_cnt = []\n",
        "    verb_cnt= []\n",
        "    adv_cnt = []\n",
        "    adj_cnt = []\n",
        "    \n",
        "    nlp = spacy.load('en_core_web_sm')\n",
        "    for essay in df['essay']:\n",
        "        parsed_essay = nlp(essay)\n",
        "        token_pos = [token.pos_ for token in parsed_essay]\n",
        "        noun_cnt.append(token_pos.count('NOUN'))\n",
        "        verb_cnt.append(token_pos.count('VERB'))\n",
        "        adv_cnt.append(token_pos.count('ADV'))\n",
        "        adj_cnt.append(token_pos.count('ADJ'))   \n",
        "    return noun_cnt, verb_cnt, adv_cnt,adj_cnt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ao8wLXxGeh9P"
      },
      "source": [
        "incorrect_spelling = get_correct_and_incorrect_spelling(train)[0]\n",
        "correct_spelling = get_correct_and_incorrect_spelling(train)[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-IHp-E4IG1nE"
      },
      "source": [
        "lexical_diversity = get_lexical_diversity(train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cuo4Cizadvvl"
      },
      "source": [
        "noun_cnt = get_list_of_number_of_pos(train)[0]\n",
        "verb_cnt = get_list_of_number_of_pos(train)[1]\n",
        "adv_cnt = get_list_of_number_of_pos(train)[2]\n",
        "adj_cnt = get_list_of_number_of_pos(train)[3]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "72faxuZenlVT"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}