{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Automated Essay Scoring V3.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sharonwu827/Deep-Learning/blob/master/Automated_Essay_Scoring_V3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T-9jqlYZCY0g",
        "outputId": "a5ffa430-1c81-4fda-8508-3a1cd883170e"
      },
      "source": [
        "#load dataset\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KIZyNyhtCdKu",
        "outputId": "870a58e5-daf3-4c4c-e181-3e0d86cee739"
      },
      "source": [
        "# import package\n",
        "import numpy as np \n",
        "import pandas as pd \n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "\n",
        "!pip install transformers\n",
        "import torch\n",
        "from transformers import BertTokenizer, BertModel\n",
        "\n",
        "# Load pre-trained model tokenizer (vocabulary)\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "\n",
        "# Keras functional API\n",
        "from keras.models import Sequential\n",
        "from keras.models import Model\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import Embedding\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import GlobalMaxPooling1D\n",
        "from keras.layers import SpatialDropout1D\n",
        "from keras.layers import Input\n",
        "\n",
        "import os\n",
        "import csv\n",
        "import copy\n",
        "import nltk\n",
        "import nltk.tokenize as tk\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "os.chdir('/content/drive/My Drive')\n",
        "nltk.download('punkt')\n",
        "\n",
        "!pip install pyenchant\n",
        "!pip install -U pip setuptools wheel\n",
        "!pip install -U spacy\n",
        "!sudo apt-get install libenchant1c2a\n",
        "import enchant #  Enchant spellchecking library\n",
        "import spacy\n",
        "import re\n",
        "from nltk import word_tokenize\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "import urllib.request\n",
        "from bs4 import BeautifulSoup"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.8.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n",
            "Requirement already satisfied: huggingface-hub==0.0.12 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.12)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.45)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.6.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from transformers) (3.13)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub==0.0.12->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.4.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "Requirement already satisfied: pyenchant in /usr/local/lib/python3.7/dist-packages (3.2.1)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
            "Requirement already satisfied: pip in /usr/local/lib/python3.7/dist-packages (21.1.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (57.1.0)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.7/dist-packages (0.36.2)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.7/dist-packages (3.1.0)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.4 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.4)\n",
            "Requirement already satisfied: typer<0.4.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.3.2)\n",
            "Requirement already satisfied: thinc<8.1.0,>=8.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy) (8.0.7)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.19.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.23.0)\n",
            "Requirement already satisfied: typing-extensions<4.0.0.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.7.4.3)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.6.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy) (57.1.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.11.3)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.5)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.8.2)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.8)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.8.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.4.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (4.41.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (20.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.4->spacy) (3.4.1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy) (2.4.7)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy) (5.1.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2021.5.30)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
            "Requirement already satisfied: click<7.2.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.4.0,>=0.3.0->spacy) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy) (2.0.1)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "libenchant1c2a is already the newest version (1.6.0-11.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 39 not upgraded.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DxKHFvCoCjCH"
      },
      "source": [
        "train = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/asap-aes/training_set_rel3.tsv\",sep='\\t', encoding='ISO-8859-1')\n",
        "dev = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/asap-aes/valid_set.tsv\",sep='\\t', encoding='ISO-8859-1')\n",
        "test = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/asap-aes/test_set.tsv\",sep='\\t', encoding='ISO-8859-1')"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "teoEWhRFES5B"
      },
      "source": [
        "prompt = pd.DataFrame({\"essay_set\":[1,2,3,4,5,6,7,8],\n",
        "                       \"prompt\":[\"More and more people use computers, but not everyone agrees that this benefits society. Those who support advances in technology believe that computers have a positive effect on people. They teach hand-eye coordination, give people the ability to learn about faraway places and people, and even allow people to talk online with other people. Others have different ideas. Some experts are concerned that people are spending too much time on their computers and less time exercising, enjoying nature, and interacting with family and friends. Write a letter to your local newspaper in which you state your opinion on the effects computers have on people. Persuade the readers to agree with you.\",\n",
        "                                \"Write a persuasive essay to a newspaper reflecting your vies on censorship in libraries. Do you believe that certain materials, such as books, music, movies, magazines, etc., should be removed from the shelves if they are found offensive? Support your position with convincing arguments from your own experience, observations, and/or reading.\",\n",
        "                                \"Write a response that explains how the features of the setting affect the cyclist. In your response, include examples from the essay that support your conclusion.\",\n",
        "                                \"Write a response that explains why the author concludes the story with this paragraph. In your response, include details and examples from the story that support your ideas.\",\n",
        "                                \"Describe the mood created by the author in the memoir. Support your answer with relevant and specific information from the memoir.\",\n",
        "                                \"Based on the excerpt, describe the obstacles the builders of the Empire State Building faced in attempting to allow dirigibles to dock there. Support your answer with relevant and specific information from the excerpt.\",\n",
        "                                \"Write about patience. Being patient means that you are understanding and tolerant. A patient person experience difficulties without complaining.Do only one of the following: write a story about a time when you were patient OR write a story about a time when someone you know was patient OR write a story in your own way about patience.\",\n",
        "                                \"We all understand the benefits of laughter. For example, someone once said, “Laughter is the shortest distance between two people.” Many other people believe that laughter is an important part of any relationship. Tell a true story in which laughter was one element or part.\"]})"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GI8lLiFQCnN3"
      },
      "source": [
        "## Data processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PjaoZsfOCkY6"
      },
      "source": [
        "def preprocess(df):\n",
        "  df['normalized_score'] = df['domain1_score'] / df.groupby('essay_set')['domain1_score'].transform('max')\n",
        "\n",
        "preprocess(train)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p4fow9XukC2T"
      },
      "source": [
        "def clean_anonymization(essay):\n",
        "  '''\n",
        "  function to remove the anoymaization\n",
        "  '''\n",
        "  res=[]\n",
        "  for i in essay.split():\n",
        "    if i.startswith(\"@\"):\n",
        "      continue\n",
        "    else:\n",
        "      res.append(i)\n",
        "  return ' '.join(res)\n",
        "\n",
        "train['essay']=train['essay'].apply(lambda x:clean_anonymization(x))\n",
        "# also remove from dev and test\n",
        "dev['essay']=dev['essay'].apply(lambda x:clean_anonymization(x))\n",
        "test['essay']=test['essay'].apply(lambda x:clean_anonymization(x))"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 366
        },
        "id": "7dabQZLdi_4A",
        "outputId": "0eb879c5-12a0-4c4b-c019-fec43ad637d2"
      },
      "source": [
        "train.head(3)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>essay_id</th>\n",
              "      <th>essay_set</th>\n",
              "      <th>essay</th>\n",
              "      <th>rater1_domain1</th>\n",
              "      <th>rater2_domain1</th>\n",
              "      <th>rater3_domain1</th>\n",
              "      <th>domain1_score</th>\n",
              "      <th>rater1_domain2</th>\n",
              "      <th>rater2_domain2</th>\n",
              "      <th>domain2_score</th>\n",
              "      <th>rater1_trait1</th>\n",
              "      <th>rater1_trait2</th>\n",
              "      <th>rater1_trait3</th>\n",
              "      <th>rater1_trait4</th>\n",
              "      <th>rater1_trait5</th>\n",
              "      <th>rater1_trait6</th>\n",
              "      <th>rater2_trait1</th>\n",
              "      <th>rater2_trait2</th>\n",
              "      <th>rater2_trait3</th>\n",
              "      <th>rater2_trait4</th>\n",
              "      <th>rater2_trait5</th>\n",
              "      <th>rater2_trait6</th>\n",
              "      <th>rater3_trait1</th>\n",
              "      <th>rater3_trait2</th>\n",
              "      <th>rater3_trait3</th>\n",
              "      <th>rater3_trait4</th>\n",
              "      <th>rater3_trait5</th>\n",
              "      <th>rater3_trait6</th>\n",
              "      <th>normalized_score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>Dear local newspaper, I think effects computer...</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>NaN</td>\n",
              "      <td>8</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.666667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>Dear I believe that using computers will benef...</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>NaN</td>\n",
              "      <td>9</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.750000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>Dear, More and more people use computers, but ...</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>NaN</td>\n",
              "      <td>7</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.583333</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   essay_id  essay_set  ... rater3_trait6  normalized_score\n",
              "0         1          1  ...           NaN          0.666667\n",
              "1         2          1  ...           NaN          0.750000\n",
              "2         3          1  ...           NaN          0.583333\n",
              "\n",
              "[3 rows x 29 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uiQ6QTXaDv26"
      },
      "source": [
        "## BERT - Sentence Embedding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fq16o4ztkO_c"
      },
      "source": [
        "### sentence embedding for semantic score"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "byg9dlwhDyvn"
      },
      "source": [
        "def bert_tokenized(df):\n",
        "  '''\n",
        "  function to Tokenize all of the sentences and map the tokens to thier word IDs.\n",
        "  '''\n",
        "  input_ids = []\n",
        "  segment_ids = []\n",
        "  attention_masks = []\n",
        "  for i in range(len(df['essay'])):\n",
        "    encoded_dict = tokenizer.encode_plus( df['essay'][i],  # Sentence to encode.\n",
        "                                         add_special_tokens=True,  # Add '[CLS]' and '[SEP]'\n",
        "                                         max_length=512,  # Pad & truncate all sentences.\n",
        "                                         pad_to_max_length=True,\n",
        "                                         return_attention_mask=True,  # Construct attn. masks.\n",
        "                                         return_tensors='pt',  # Return pytorch tensors.\n",
        "                                         )\n",
        "    input_ids.append(encoded_dict['input_ids'])\n",
        "    segment_ids.append(encoded_dict['token_type_ids'])\n",
        "    # And its attention mask (simply differentiates padding from non-padding).\n",
        "    attention_masks.append(encoded_dict['attention_mask'])\n",
        "\n",
        "  # Convert the lists into tensors (constructed ).\n",
        "  input_ids = torch.cat(input_ids, dim=0)\n",
        "  segment_ids = torch.cat(segment_ids, dim=0)\n",
        "  attention_masks = torch.cat(attention_masks, dim=0)\n",
        "  return input_ids, segment_ids, attention_masks\n",
        "\n",
        "\n",
        "\n",
        "def bert_model(input_id,attention_masks,segment_ids):\n",
        "\n",
        "  '''\n",
        "  funtion to run the text through BERT, and collect all of the hidden states produced from all 12 layers. \n",
        "  '''\n",
        "\n",
        "  # Load pre-trained model (weights)\n",
        "  model = BertModel.from_pretrained('bert-base-uncased',\n",
        "                                  output_hidden_states = True # Whether the model returns all hidden-states.\n",
        "                                  )\n",
        "  # Put the model in \"evaluation\" mode, meaning feed-forward operation, and turns off dropout regularization\n",
        "  model.eval()\n",
        "\n",
        "  with torch.no_grad():\n",
        "    outputs = model(input_ids = input_id, attention_mask = attention_masks, token_type_ids = segment_ids)\n",
        "    # https://huggingface.co/transformers/model_doc/bert.html#bertmodel\n",
        "    hidden_states = outputs[2] # The full set of hidden states for this model, stored in the object hidden_statesprint (\"Number of layers:\", len(hidden_states), \"  (initial embeddings + 12 BERT layers)\")\n",
        "    last_hidden_states = outputs[0]  # get the last hidden state\n",
        "  \n",
        "  #print (\"Number of layers:\", len(hidden_states), \"  (initial embeddings + 12 BERT layers)\")\n",
        "  #layer_i = 0\n",
        "  \n",
        "  # the batch size, is used when submitting multiple sentences to the model at once\n",
        "  #print (\"Number of batches:\", len(hidden_states[layer_i]))\n",
        "  \n",
        "  #batch_i = 0\n",
        "  #print (\"Number of tokens:\", len(hidden_states[layer_i][batch_i]))\n",
        "  #token_i = 0\n",
        "  \n",
        "  #print (\"Number of hidden units:\", len(hidden_states[layer_i][batch_i][token_i]))\n",
        "  \n",
        "  # the output of last hidden states\n",
        "  sentence_embedding = last_hidden_states[:,0,:]\n",
        "  return sentence_embedding"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c8WR0sFmGJf5",
        "outputId": "45e9e2fd-c41d-4b17-c587-165061923fa3"
      },
      "source": [
        "input_ids_ss = bert_tokenized(train)[0]\n",
        "segment_ids_ss = bert_tokenized(train)[1]\n",
        "attention_masks_ss = bert_tokenized(train)[2]"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pI030TG_Me3h"
      },
      "source": [
        "# sentence embedding for semantic score\n",
        "sentence_embedding_ss = bert_model(input_ids_ss,segment_ids_ss,attention_masks_ss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X0Fb87jekVU-"
      },
      "source": [
        "### sentence embedding for prompt relevant score"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 417
        },
        "id": "jRC5RteokU4I",
        "outputId": "bcfd1027-5f9f-4b3c-d75c-17d6a3d8f5f5"
      },
      "source": [
        "train_ps=train.merge(prompt,on='essay_set',how='left')\n",
        "train_ps.head(3)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>essay_id</th>\n",
              "      <th>essay_set</th>\n",
              "      <th>essay</th>\n",
              "      <th>rater1_domain1</th>\n",
              "      <th>rater2_domain1</th>\n",
              "      <th>rater3_domain1</th>\n",
              "      <th>domain1_score</th>\n",
              "      <th>rater1_domain2</th>\n",
              "      <th>rater2_domain2</th>\n",
              "      <th>domain2_score</th>\n",
              "      <th>rater1_trait1</th>\n",
              "      <th>rater1_trait2</th>\n",
              "      <th>rater1_trait3</th>\n",
              "      <th>rater1_trait4</th>\n",
              "      <th>rater1_trait5</th>\n",
              "      <th>rater1_trait6</th>\n",
              "      <th>rater2_trait1</th>\n",
              "      <th>rater2_trait2</th>\n",
              "      <th>rater2_trait3</th>\n",
              "      <th>rater2_trait4</th>\n",
              "      <th>rater2_trait5</th>\n",
              "      <th>rater2_trait6</th>\n",
              "      <th>rater3_trait1</th>\n",
              "      <th>rater3_trait2</th>\n",
              "      <th>rater3_trait3</th>\n",
              "      <th>rater3_trait4</th>\n",
              "      <th>rater3_trait5</th>\n",
              "      <th>rater3_trait6</th>\n",
              "      <th>normalized_score</th>\n",
              "      <th>prompt</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>Dear local newspaper, I think effects computer...</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>NaN</td>\n",
              "      <td>8</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>More and more people use computers, but not ev...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>Dear I believe that using computers will benef...</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>NaN</td>\n",
              "      <td>9</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.750000</td>\n",
              "      <td>More and more people use computers, but not ev...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>Dear, More and more people use computers, but ...</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>NaN</td>\n",
              "      <td>7</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.583333</td>\n",
              "      <td>More and more people use computers, but not ev...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   essay_id  ...                                             prompt\n",
              "0         1  ...  More and more people use computers, but not ev...\n",
              "1         2  ...  More and more people use computers, but not ev...\n",
              "2         3  ...  More and more people use computers, but not ev...\n",
              "\n",
              "[3 rows x 30 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j3lspd-BlTzC"
      },
      "source": [
        "def generate_input_ids(df):\n",
        "  encoded_dict = tokenizer.encode_plus(df[\"prompt\"], df[\"essay\"],\n",
        "                                       max_length=512,  # Pad & truncate all sentences\n",
        "                                       pad_to_max_length=True,\n",
        "                                       add_special_tokens = True,\n",
        "                                       # return_tensors='np'  # Return pytorch tensors.\n",
        "                                       )\n",
        "  return encoded_dict['input_ids']\n",
        "\n",
        "def generate_attention_masks(df):\n",
        "  encoded_dict = tokenizer.encode_plus(df[\"prompt\"], df[\"essay\"],\n",
        "                                       max_length=512,  # Pad & truncate all sentences\n",
        "                                       pad_to_max_length=True,\n",
        "                                       add_special_tokens = True,\n",
        "                                       # return_tensors='np'  # Return pytorch tensors.\n",
        "                                       )\n",
        "  return encoded_dict['attention_mask']"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YjW2QtHUkgLA"
      },
      "source": [
        "train_ps['input_ids_ps'] = sub1.apply(generate_input_ids, axis=1)\n",
        "train_ps['attention_masks_ps'] = sub1.apply(generate_attention_masks, axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q6b6t9OnkgTA"
      },
      "source": [
        "# Search the input_ids for the first instance of the `[SEP]` token.\n",
        "def segment_ids(df):\n",
        "  for input_id in sub1['input_ids']:\n",
        "    sep_index = []\n",
        "    sep_index.append(input_ids.index(tokenizer.sep_token_id))\n",
        "    # The number of segment A tokens includes the [SEP] token istelf\n",
        "    for index in sep_index:\n",
        "      segment_ids = []\n",
        "      num_seg_a = index + 1\n",
        "      # The remainder are segment B\n",
        "      num_seg_b = len(input_ids) - num_seg_a\n",
        "      # Construct the list of 0s and 1s.\n",
        "      segment_ids = [0]*num_seg_a + [1]*num_seg_b\n",
        "  return segment_ids"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bD4kY3iPli7H"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XMY-caAXGKVD"
      },
      "source": [
        "## LSTM "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SfXJPyleFZRW"
      },
      "source": [
        "def lstm_model(batch_size, pochs, max_words,output_dim,max_phrase_len,input_ids, validation_split):\n",
        "\n",
        "  # create the Keras model\n",
        "  lstm = Sequential()\n",
        "  lstm.add(Embedding(input_dim = max_words\n",
        "                     ,output_dim = output_dim\n",
        "                     ,input_length = max_phrase_len))\n",
        "  lstm.add(SpatialDropout1D(0.3))\n",
        "  lstm.add(LSTM(256, dropout = 0.3, recurrent_dropout = 0.3))\n",
        "  lstm.add(Dense(256, activation = 'relu'))\n",
        "  lstm.add(Dropout(0.3))\n",
        "  \n",
        "  # To finish off our network, we’ll add a standard fully-connected (Dense) layer \n",
        "  # and an output layer with sigmoid activation:     \n",
        "  lstm.add(Dense(64, activation=\"relu\"))\n",
        "  lstm.add(Dense(1, activation=\"sigmoid\"))\n",
        "  \n",
        "  # Compile the model: before training a model, to configure the learning process, which is done via the compile method. \n",
        "  lstm.compile(optimizer='rmsprop',\n",
        "               loss='mse',\n",
        "               metrics=['accuracy']) # for a mean squared error regression problem, https://faroit.com/keras-docs/1.2.0/getting-started/sequential-model-guide/\n",
        "  \n",
        "  print(lstm.summary())\n",
        "  \n",
        "  history = lstm.fit(input_ids.numpy(),labels,\n",
        "                     validation_split = validation_split,\n",
        "                     epochs = epochs,\n",
        "                     batch_size = batch_size)\n",
        "  Score = lstm.predict(input_ids)\n",
        "  return Score\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sz-9TfCeGOt0"
      },
      "source": [
        "## Handcrafted Features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LRRKa3ZJGUGl"
      },
      "source": [
        "def get_correct_and_incorrect_spelling(df):\n",
        "  \"\"\"\n",
        "  Function that measures lexical diversity which is the ratio of total words to unique words\n",
        "  \"\"\"\n",
        "  # Load spaCy model\n",
        "  nlp = spacy.load('en_core_web_sm')\n",
        "  # load the enchant dictionary\n",
        "  d = enchant.Dict(\"en_US\")\n",
        "  \n",
        "  incorrect_spelling = []\n",
        "  correct_spelling = []\n",
        "  \n",
        "  for essay in df['essay']:\n",
        "    individual = []\n",
        "    for word in essay.split():\n",
        "      individual.append(d.check(word))\n",
        "    \n",
        "    output.append((individual.count(False), individual.count(True)))\n",
        "  return output\n",
        "\n",
        "\n",
        "def get_lexical_diversity(df):\n",
        "    \"\"\"\n",
        "    Function that measures lexical diversity which is\n",
        "    The ratio of total words to unique words\n",
        "    \"\"\"\n",
        "    diversity = []\n",
        "    for essay in df['essay']:\n",
        "      diversity.append(round(len(tk.word_tokenize(essay)) / float(len(set(tk.word_tokenize(essay)))), 2))\n",
        "    return diversity\n",
        "\n",
        "\n",
        "def get_list_of_number_of_pos(df):\n",
        "    \"\"\"\n",
        "    Function that parses the essay for each words POS\n",
        "    Returns tuples containg for now, nouns, verbs, adverbs and adjectives\n",
        "    \"\"\"\n",
        "    pos = []\n",
        "    \n",
        "    for essay in df['essay']:\n",
        "        parsed_essay = nlp(essay)\n",
        "        token_pos = [token.pos_ for token in parsed_essay]\n",
        "        \n",
        "        pos.append((token_pos.count('NOUN'), token_pos.count('VERB'), token_pos.count('ADV'), token_pos.count('ADJ')))\n",
        "        \n",
        "    return pos"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-IHp-E4IG1nE"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}